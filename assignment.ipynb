{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c73134",
   "metadata": {},
   "source": [
    "# Team 2 - NUS ISS Assignement - RAG \n",
    "\n",
    "Context: We want to build a LLM using RAG Bookstore to recommend users books to read accordingly to their interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f71b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "from smolagents import tool, Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206a5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up of the LLM Model \n",
    "# We Are using google/flan-t5-small\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content from the EPUB file\n",
    "# epub_loader = UnstructuredEPubLoader(file_path='data/charles-dickens_a-christmas-carol.epub')\n",
    "# epub_loader = UnstructuredEPubLoader(file_path='data/the_gift_of_the_magi.epub')\n",
    "# epub_loader = UnstructuredEPubLoader(file_path='data/the_happy_prince.epub')\n",
    "epub_loader = UnstructuredEPubLoader(file_path='data/the_nightingale_and_the_rose.epub')\n",
    "doc = epub_loader.load()\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chunk size and overlap for text splitting\n",
    "# This will determine how the text is divided into smaller segments for processing\n",
    "# Adjust these values based on your specific requirements\n",
    "chunks_size = 1024\n",
    "chunks_overlap = 50\n",
    "\n",
    "# Split the text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunks_size,\n",
    "    chunk_overlap=chunks_overlap\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "# This will create smaller text segments that can be processed by the model\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First chunk: {chunks[0].page_content[:500]}...\")  # Display the first 500 characters of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding model\n",
    "embed_model_name = 'BAAI/bge-small-en-v1.5'\n",
    "embed_model_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=embed_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c44138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the chunks for inserting into Chroma\n",
    "texts = [d.page_content for d in chunks]\n",
    "\n",
    "# Generate PK foreach text chunk\n",
    "texts_ids = [str(uuid4())[:8] for _ in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ephemeral Chroma client and save chunks\n",
    "collection_name = 'epub'\n",
    "\n",
    "# Create a Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a embeeding function\n",
    "embed_model_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "\n",
    "try:\n",
    "    # Clean up collection\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new collection with the specified name and embedding function\n",
    "collection = chroma_client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embed_model_func\n",
    ")\n",
    "\n",
    "# If the document <=0, than load chunks into the collection\n",
    "if collection.count() == 0:\n",
    "    print(\"Inserting chunks document into Chroma collection...\")\n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        ids=texts_ids\n",
    "    )\n",
    "\n",
    "print(f\"Number of documents in collection '{collection_name}': {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0725372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeneraetionConfig for the model\n",
    "config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    temperature= 0.7,\n",
    "    top_k= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66175c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to pass query to model and get response\n",
    "def query_model(query, config):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, generation_config=config) # **inputs represents the input tensors (inputs_ids)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c09e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool for Agent to use\n",
    "@tool\n",
    "def agent_query_tool(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform a chroma query on the book tables. Return the result as an array of records.\n",
    "    The table has the following columns:\n",
    "    - id: str\n",
    "    - title: str\n",
    "    - author: str\n",
    "    - summary: str\n",
    "    - content: str\n",
    "\n",
    "    Args:\n",
    "        <To be changed based on starting data injected in>\n",
    "        prompt: The user's question about the book title, author, author, summary and content.\n",
    "\n",
    "    Returns:\n",
    "        <To be changed based on starting data injected in>\n",
    "        str: list of tuple. Each element corresponds to a record from the query.\n",
    "\n",
    "    Example:\n",
    "        <To be changed based on starting data injected in>\n",
    "        result = query_tool(\"Who is the author?\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of top results to return\n",
    "    top_k = 5 \n",
    "\n",
    "    # Query the collection for relevant documents\n",
    "    collection_query = collection.query(\n",
    "        query_texts = [prompt],\n",
    "        n_results = top_k # Number of top results to return\n",
    "    )\n",
    "\n",
    "    # Build context from the top results\n",
    "    context= \"\"\n",
    "    for id in collection_query['ids'][0]:\n",
    "        doc = collection.get(ids=[id])\n",
    "        context += doc['documents'][0] + \"\\n\"\n",
    "\n",
    "    # Enrich the prompt with the context\n",
    "    enriched_prompt = (\n",
    "        f\"Answer based on context:\\n\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        f\"Top {top_k} results from the ChromaDb database based on the question\\n\"\n",
    "        f\"{prompt}\"\n",
    "    )\n",
    "    # print(enriched_prompt)\n",
    "\n",
    "    # Pass the prompt to the model\n",
    "    response = query_model(enriched_prompt, config)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tool\n",
    "prompt = \"Who is Scrooge?\"\n",
    "result = agent_query_tool(prompt)\n",
    "print(f\"Model response: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
