{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc61d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages \n",
    "# Run this if your kernel packages is empty.\n",
    "# Leverage on https://colab.research.google.com/github/GSCCLIVE/nus-iss-rag-assignment/blob/main/test.ipynb#scrollTo=eufh-4yZlCOn\n",
    "!pip3 install --upgrade pip\n",
    "!pip install chromadb pypandoc\n",
    "!pip install transformers datasets evaluate rouge_score loralib peft \n",
    "!pip3 install ipykernel ipywidgets \n",
    "!pip3 install langchain-community sentence-transformers unstructured\n",
    "!pip3 install diffusers accelerate scipy safetensors\n",
    "!pip3 install torch torchdata torchvision\n",
    "!pip3 install smolagents openai\n",
    "!pip3 install nbconvert[webpdf]\n",
    "!pip3 huggingface_hub[hf_xet]\n",
    "\n",
    "!pip3 install unstructured \n",
    "!pip3 install pandas networkx openpyxl\n",
    "!pip3 install python-magic python-pptx\n",
    "!pip3 install docx2txt docx\n",
    "!pip3 install jq nltk\n",
    "!pip3 install duckduckgo_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d1f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pprint\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loading Dataset with meta data\n",
    "dataset = load_dataset(\"IsmaelMousa/books\", split=\"train\")\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86549747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the embedding model\n",
    "embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f60d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk only the text and add chunks with metadata to ChromaDB\n",
    "\n",
    "chunks_size = 1024\n",
    "chunks_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunks_size,\n",
    "    chunk_overlap=chunks_overlap\n",
    ")\n",
    "\n",
    "all_chunk_texts = []\n",
    "all_chunk_metadatas = []\n",
    "all_chunk_ids = []\n",
    "\n",
    "for i, row in enumerate(dataset):\n",
    "    # Chunk only the book text\n",
    "    chunks = text_splitter.split_text(row['EN'])\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        all_chunk_texts.append(chunk)\n",
    "        all_chunk_metadatas.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"author\": row[\"author\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        })\n",
    "        all_chunk_ids.append(f\"{i}_{j}\")\n",
    "        \n",
    "# check the count of chunks\n",
    "print(len(all_chunk_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4778392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ChromaDB collection and add chunks\n",
    "collection_name = 'books'\n",
    "client = chromadb.Client()\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embed_model,\n",
    ")\n",
    "\n",
    "batch_size = 128  # Number of rows to process at once\n",
    "chunk_batch_size = 128  # Number of chunks to add at once\n",
    "\n",
    "for batch_start in range(0, len(dataset), batch_size):\n",
    "    batch_end = min(batch_start + batch_size, len(dataset))\n",
    "    batch = dataset.select(range(batch_start, batch_end))\n",
    "    all_chunk_texts = []\n",
    "    all_chunk_metadatas = []\n",
    "    all_chunk_ids = []\n",
    "    for i, row in enumerate(batch, start=batch_start):\n",
    "        chunks = text_splitter.split_text(row['EN'])\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            all_chunk_texts.append(chunk)\n",
    "            all_chunk_metadatas.append({\n",
    "                \"title\": row[\"title\"],\n",
    "                \"author\": row[\"author\"],\n",
    "                \"category\": row[\"category\"]\n",
    "            })\n",
    "            all_chunk_ids.append(f\"{i}_{j}\")\n",
    "            # Add to collection in sub-batches\n",
    "            if len(all_chunk_texts) >= chunk_batch_size:\n",
    "                collection.add(\n",
    "                    documents=all_chunk_texts,\n",
    "                    ids=all_chunk_ids,\n",
    "                    metadatas=all_chunk_metadatas\n",
    "                )\n",
    "                all_chunk_texts = []\n",
    "                all_chunk_metadatas = []\n",
    "                all_chunk_ids = []\n",
    "    # Add any remaining chunks in this batch\n",
    "    if all_chunk_texts:\n",
    "        collection.add(\n",
    "            documents=all_chunk_texts,\n",
    "            ids=all_chunk_ids,\n",
    "            metadatas=all_chunk_metadatas\n",
    "        )\n",
    "    print(f\"Processed batch {batch_start} to {batch_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c05d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up of the LLM Model \n",
    "# We Are using google/flan-t5-small\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab72329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the genre of the book 'The Picture of Dorian Gray'?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "pp.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
